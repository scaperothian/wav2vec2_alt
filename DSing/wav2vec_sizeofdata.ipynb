{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2ea95bf-371e-4183-b9c2-e1900e435f79",
   "metadata": {},
   "source": [
    "# Fine Tuning Pre-trained WAV2VEC Using DSing Train\n",
    "\n",
    "## Summary\n",
    "\n",
    "1. Loading WAV2VEC model\n",
    "2. Creating a Huggingface dataset for batch processing by Cropping the DSing Dataset described below (8 songs for each split, 1.5seconds max)\n",
    "\n",
    "The dataset is cropped to allow for processing examples for this notebook.  in the actual processing, it will look different.\n",
    "\n",
    "## DSing Dataset \n",
    "\n",
    "Reading data from DSing Dataset.  Filesystem formatted this way to convert easily to huggingface dataset.\n",
    "\n",
    "where: <br>\n",
    "dev/test/trainX are datasets split.<br>\n",
    "\\[split\\]_text contains transcript for the snippet.<br>\n",
    "\\[split\\]_spk2gender contains information about gender for snippet.<br>\n",
    "\n",
    "Tests Split: 480 Utterances, 48 minutes<br>\n",
    "Dev Split: 482 Utterances, 41 minutes<br>\n",
    "Train1 Split: 8794 Utterances, 15.1 hours<br>\n",
    "Train3 Split: 25526 Utterances, 44.7 hours<br>\n",
    "Train30 Split: 268,392 Utterances, 149.1 hours<br>\n",
    "\n",
    "sing_300x30x2/dataset/<br>\n",
    "├── dev/<br>\n",
    "├───| metadata.csv<br>\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&nbsp;| \\<audio files\\>.wav<br>\n",
    "├── dev_spk2gender<br>\n",
    "├── dev_text<br>\n",
    "├── test/<br>\n",
    "├───| metadata.csv<br>\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&nbsp;| \\<audio files\\>.wav<br>\n",
    "├── test_spk2gender<br>\n",
    "├── test_text<br>\n",
    "├── train1/<br>\n",
    "├───| metadata.csv<br>\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&nbsp;| \\<audio files\\>.wav<br>\n",
    "├── train1_spk2gender<br>\n",
    "├── train1_text<br>\n",
    "├── train3/<br>\n",
    "├───| metadata.csv<br>\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&nbsp;| \\<audio files\\>.wav<br>\n",
    "├── train3_spk2gender<br>\n",
    "└── train3_text<br>\n",
    "├── train30/<br>\n",
    "├───| metadata.csv<br>\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&nbsp;| \\<audio files\\>.wav<br>\n",
    "├── train30_spk2gender<br>\n",
    "├── train30_text<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f7c3a-68b3-482e-b9ad-fcfe00fa7378",
   "metadata": {},
   "source": [
    "# For Colab Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f30e8c-4202-4853-9233-8edb693c8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f6466e-8da6-4ecf-9cee-87173a88874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip /content/drive/MyDrive/damp_dataset.zip -d /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898177fe-bc08-44c2-ba84-c591b015bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==4.39.1\n",
    "#!pip install accelerate==0.28.0\n",
    "#!pip install evaluate\n",
    "#!pip install torchaudio\n",
    "#!pip install flashlight-text\n",
    "#!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35463b93-1c43-4677-989f-872cdd111097",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a8a1c-9b3b-4c83-8e08-bad91e8f9406",
   "metadata": {},
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e74fbe0-890b-4665-b329-6cfa0a89387c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import evaluate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import IPython\n",
    "\n",
    "from IPython.display import display, HTML, Audio\n",
    "\n",
    "from datasets import load_dataset, load_metric, ClassLabel, DatasetDict\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchaudio.models.decoder import ctc_decoder\n",
    "from torchaudio.models.decoder import download_pretrained_files\n",
    "\n",
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import pipeline\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb2ce7f-f7b7-400b-ab79-5fd4746580c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Dataset unpacked and preprocessed into child folder dataset/\n",
    "# DSing is around 1.9GB after being preprocessed.\n",
    "#\n",
    "tokens_file=\"./tokens.txt\"\n",
    "dataset_folder = \"../sing_300x30x2/damp_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "257c0ca5-72fe-4d09-bedb-b91d65461806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-large-960h-lv60-self were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline\n",
    "model_checkpoint=\"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "asr_pipeline = pipeline(\"automatic-speech-recognition\", model=model_checkpoint)\n",
    "model = asr_pipeline.model\n",
    "\n",
    "# Note: I want to use a beam search library that has been pre-trained on librispeech.  the pretrained beam search \n",
    "#       has been created with all lowercase characters (i.e. phoneme settings, etc.).  I am changing the default \n",
    "#       model tokenizer vocab settings to accept lower case inputs so i can use the decoder.  However, it will also\n",
    "#       accept upper case, so this may also impact performance but only for inference.  (i.e. not sure where this \n",
    "#       is normalized to .\n",
    "asr_pipeline.tokenizer.do_lower_case = True\n",
    "\n",
    "target_sampling_rate = asr_pipeline.feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bef0f166-627b-4fd7-877e-a505fc090248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"return_attention_mask\": true,\n",
       "  \"sampling_rate\": 16000\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_pipeline.feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7577ca1d-5658-46ef-a45e-34518220c960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2CTCTokenizer(name_or_path='facebook/wav2vec2-large-960h-lv60-self', vocab_size=32, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
       "\t1: AddedToken(\"<s>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
       "\t2: AddedToken(\"</s>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_pipeline.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabfa88c-ac3e-41be-8d91-5768de96907f",
   "metadata": {},
   "source": [
    "## Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b01202ef-2b0a-4824-a191-bb75f4469849",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = download_pretrained_files(\"librispeech-4-gram\")\n",
    "# Found from Fairseq for Wav2Vec2 - \n",
    "# https://github.com/facebookresearch/fairseq/blob/main/examples/wav2vec/config/finetuning/vox_960h_2_aws.yaml\n",
    "# Note: I am not using the same lexicon file though...\n",
    "# LM_WEIGHT = 2.0\n",
    "# WORD_SCORE = 0\n",
    "# SIL_SCORE = -1\n",
    "\n",
    "LM_WEIGHT = 3.23\n",
    "WORD_SCORE = -0.26\n",
    "SIL_SCORE = 0\n",
    "\n",
    "beam_search_decoder = ctc_decoder(\n",
    "    lexicon=files.lexicon,\n",
    "    tokens=tokens_file,\n",
    "    lm=files.lm,\n",
    "    nbest=1,\n",
    "    beam_size=1500,\n",
    "    lm_weight=LM_WEIGHT,\n",
    "    word_score=WORD_SCORE,\n",
    "    sil_score=SIL_SCORE,\n",
    "    blank_token='<pad>',\n",
    "    unk_word='<unk>'\n",
    ")\n",
    "\n",
    "greedy_decoder = ctc_decoder(\n",
    "    lexicon=files.lexicon,\n",
    "    tokens=tokens_file,\n",
    "    lm=files.lm,\n",
    "    nbest=1,\n",
    "    beam_size=1,\n",
    "    lm_weight=LM_WEIGHT,\n",
    "    word_score=WORD_SCORE,\n",
    "    sil_score=SIL_SCORE,\n",
    "    blank_token='<pad>',\n",
    "    unk_word='<unk>'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b588600-dea7-4b93-bb72-a69409732152",
   "metadata": {},
   "source": [
    "# Generate Hugginface Dataset Object from DAMP 300x30x2 dataset\n",
    "\n",
    "Reference: https://huggingface.co/docs/datasets/audio_load\n",
    "\n",
    "Other pre-processing is heavily inspired from: https://colab.research.google.com/drive/1nCC5Ci-81U5opK_VuXDiZlmcAuATreF2#scrollTo=RBDRAAYxRE6n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1204283b-7ad5-4d1f-9115-e4fa40e64680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c34e252fbe4b0e8fe9a50f18d5c022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/10187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read labels for all song utterances for DSING splits (test, dev (aka validation), train1 (aka train)\n",
    "\n",
    "# This object will create a datasetDict for train, validation and test\n",
    "dali = load_dataset(\"audiofolder\", data_dir=\"../dali-dataset/dali_datasets\")\n",
    "\n",
    "# Note: creating small dataset objects to ensure training goes well.\n",
    "#       However, a limitation of using the audiofolder is that I cannot specify how much of\n",
    "#       the data to read in.\n",
    "#dsing_small_train, dsing_small_val, dsing_small_test = load_dataset(\"audiofolder\", data_dir=dataset_folder, split=['train[:8]','validation[:8]','test[:8]'])\n",
    "# When using split with a list, the return objects are DataSet objects of size of list.\n",
    "# Recasting back to DatasetDict for ease of use downstream.\n",
    "#dsing = DatasetDict({\"train\": dsing_small_train, \"validation\": dsing_small_val, \"test\":dsing_small_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a2a71de-3111-43e9-858f-9b5a2cae5f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'transcription'],\n",
       "        num_rows: 10186\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dali"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54138f53-0a58-42ac-9fd9-2acf6b8ee86f",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28c8a8ab-7d5d-4e69-8868-cac2cc4b32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch,tokenizer,feature_extractor):\n",
    "    \"\"\"\n",
    "    Creating a new dataset with the map function to generate the \n",
    "    keys below.  Padding will occur in the data collator on a per\n",
    "    batch basis. \n",
    "\n",
    "    Inputs (i.e. feature extractor):\n",
    "    input_values   - tensor array for audio samples (shape=(n,) - where n is the number of audio samples)\n",
    "    attention_mask - used for expressing where there are padded samples \n",
    "\n",
    "    Outputs (i.e. tokenizer related)\n",
    "    labels - tensor array for text output tokens (i.e. not transcript).  (shape=(m,) - where m is the number of character tokens)\n",
    "    \"\"\"\n",
    "    chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
    "    batch[\"transcription\"] = re.sub(chars_to_ignore_regex, '', batch[\"transcription\"]).lower() + \" \"\n",
    "\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "\n",
    "    # Feature Extractor manipulation\n",
    "    #\n",
    "    # this object will return a list of lists because the \n",
    "    # transcriptions are not padded (i.e. as opposed to a \n",
    "    # Tensor of tensors when using return_tensors='pt').\n",
    "    # Padding is done per batch to optimize the size for inference and \n",
    "    # training.\n",
    "    #\n",
    "    # data_collator is responsible for padding the data.\n",
    "    inputs_values_pt = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"])\n",
    "    batch[\"attention_mask\"] = inputs_values_pt.attention_mask\n",
    "    batch[\"input_values\"] = inputs_values_pt.input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    \n",
    "    # Tokenizer manipulation\n",
    "    #\n",
    "    # this object will return a list of lists because the \n",
    "    # transcriptions are not padded (i.e. as opposed to a \n",
    "    # Tensor of tensors when using return_tensors='pt').\n",
    "    # Padding is done per batch to optimize the size for inference and \n",
    "    # training.\n",
    "    #\n",
    "    # data_collator is responsible for padding the data.\n",
    "    labels_pt = tokenizer(batch[\"transcription\"])\n",
    "    batch[\"labels\"] = labels_pt['input_ids']\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbb1eaff-b6ea-4d39-8009-515cecb6ca9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f77dce7d1f41a6a24163be942e8fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8794 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29af007e589c45709724a9e062e86456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/482 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090cd13728c849b0bce39e339b996104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create dataset keys that are expected by model for each split.\n",
    "dsing_dataset = dsing.map(prepare_dataset, remove_columns=[\"audio\",\"transcription\"], fn_kwargs={'tokenizer':asr_pipeline.tokenizer, 'feature_extractor':asr_pipeline.feature_extractor})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4694e0b0-dded-4dea-b4d5-8faa81570f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the already tokenized inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "\n",
    "            Other Options in the pad method that are NOT implemented for this class (i.e. I always want to pad to longest for the \n",
    "            input and the labels)\n",
    "            * (not implemented) :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * (not implemented) :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "\n",
    "    Reference Code here:\n",
    "    https://huggingface.co/blog/fine-tune-wav2vec2-english\n",
    "\n",
    "    \n",
    "    Note: in the example referenced above, there were parameters for padding max length, etc.  I have created some logic \n",
    "    in the prepare_dataset to support truncation of data for testing and benchmarking.  I do not think i need max_length \n",
    "    options for collator at this time.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer: Wav2Vec2CTCTokenizer\n",
    "    feature_extractor: Wav2Vec2FeatureExtractor\n",
    "    padding: Union[bool, str] = \"longest\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        # Features in this case is a list of batch size that contains DataSet objects from the train split\n",
    "        # (including pretokenized labels). the output batch has been changed from a list back to a dictionary \n",
    "        # with the respective data objects.\n",
    "        #\n",
    "        # Note for future self: \n",
    "        # pad is being called from PreTrainedTokenizerBase.pad.  From docs:\n",
    "        #      Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length\n",
    "        #      in the batch.\n",
    "        #      \n",
    "        #    Padding side (left/right) padding token ids are defined at the tokenizer level (with `self.padding_side`,\n",
    "        #    `self.pad_token_id` and `self.pad_token_type_id`).\n",
    "\n",
    "        #    Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the\n",
    "        #    text followed by a call to the `pad` method to get a padded encoding.\n",
    "        # \n",
    "        #         <Tip>\n",
    "        # \n",
    "        #         If the `encoded_inputs` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the\n",
    "        #         result will use the same type unless you provide a different tensor type with `return_tensors`. In the case of\n",
    "        #         PyTorch tensors, you will lose the specific device of your tensors however.\n",
    "        # \n",
    "        #         </Tip>\n",
    "\n",
    "        # Audio Input Data (not tokenized)\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "\n",
    "        # batch is a dictionary-like type.\n",
    "        batch = self.feature_extractor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # Tokenized Transcript Labels (character level tokens)\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.tokenizer.pad(\n",
    "            label_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        batch[\"labels\"] = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "437add63-76e7-4560-a9d4-bdef7f4457c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(eval_pred,kind='beam',compute=True):\n",
    "    \"\"\"\n",
    "    Calculates WER for a batch of logits and their labels.\n",
    "\n",
    "    eval_pred - tuple (logit output from the model, token labels from dataset)\n",
    "    kind - can compare between beam search and greedy search.  both using kenlm \n",
    "\n",
    "    compute - bool - for training this will compute WER every time its logged.  \n",
    "                     this is nice for understanding if the training is working.\n",
    "                     for evaluation, this is set to false so compute is run after\n",
    "                     all batches are processed.\n",
    "\n",
    "    output is the WER computed from the batch.  if the model is run multiple times, the \n",
    "    batch WERs are aggregated.\n",
    "\n",
    "    Note: add_batch and then doing compute will clear the previously cached batch results.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    #print(f\"Logit Type: {type(logits)}\")\n",
    "\n",
    "    # In some scenarios, the input the compute_metrics is a tensor.\n",
    "    if type(logits) is np.ndarray:\n",
    "        logits = torch.Tensor(logits)\n",
    "    else:\n",
    "        # copy this tensor for computing things...\n",
    "        logits = logits.clone().detach().requires_grad_(False)    \n",
    "    #print(f\"Changing Logit Type to: {type(logits)}\")\n",
    "    #print(f\"{logits.shape}\")\n",
    "    \n",
    "    if kind=='beam':\n",
    "        # Creates a list of lists that are of size [batch_size,emissions,vocab_size]\n",
    "        #\n",
    "        # Where output[0][0] gives you the CTCHypothesis object.\n",
    "        #\n",
    "        # Extract transcript from output[0][0].words (i.e. list of words).  \n",
    "        # May need to join depending on objective.\n",
    "        #\n",
    "        predictions = beam_search_decoder(logits)\n",
    "    elif kind=='greedy':\n",
    "        # Creates a list of lists that are of size [batch_size,1]\n",
    "        #\n",
    "        # Where output[0][0] gives you the CTCHypothesis object.\n",
    "        #\n",
    "        # Extract transcript from output[0][0].words (i.e. list of words).  \n",
    "        # May need to join depending on objective.\n",
    "        #\n",
    "        predictions = greedy_decoder(logits)\n",
    "    else:\n",
    "        print(f\"Error passing in decoder kind: {kind}\")\n",
    "        sys.exit()\n",
    "\n",
    "    ref = asr_pipeline.tokenizer.batch_decode(labels)\n",
    "    pred = [\" \".join(prediction[0].words) for prediction in predictions]\n",
    "\n",
    "    wer_metric.add_batch(predictions=pred, references=ref)\n",
    "\n",
    "    if compute: \n",
    "        return {\"wer\":wer_metric.compute()}\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1b1bb225-d7ff-4c6f-9ea3-06c6ecb84dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO\n",
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "#logging.set_verbosity_info()\n",
    "#logger = logging.get_logger(\"transformers\")\n",
    "#logger.warning(\"INFO\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    overwrite_output_dir = True,\n",
    "    # gradient_checkpointing\n",
    "    # the trade off is  O(sqrt(n)) savings \n",
    "    # with implemented memory-wise, at the \n",
    "    # cost of performing one additional \n",
    "    # forward pass.\n",
    "    gradient_checkpointing=True,   \n",
    "    use_cpu=True,\n",
    "    #fp16=True,                      #use when we are doing the GPU based training\n",
    "    #resume_from_checkpoint=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=1,\n",
    "    eval_steps=1,\n",
    "#    logging_dir='./logs',\n",
    "    learning_rate=1e-4,              # Based on fairseq yaml\n",
    "    weight_decay=0.005,\n",
    "    warmup_steps=1,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "#    report_to='all',                 #logging thing.  there was a warning.\n",
    "#    logging_steps=1,\n",
    "#    logging_strategy='steps',\n",
    "#    log_level='warning'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "89569f08-4ed0-4963-a270-322158ebf2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/das/Library/Caches/pypoetry/virtualenvs/wav2vec-dsing-E_qMqI5b-py3.11/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.freeze_feature_encoder()\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(\n",
    "    tokenizer=asr_pipeline.tokenizer, \n",
    "    feature_extractor=asr_pipeline.feature_extractor, \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dsing_dataset['train'],\n",
    "    eval_dataset=dsing_dataset['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=asr_pipeline.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "799bf146-2c97-440f-91b4-99d4d2bdef50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/das/Library/Caches/pypoetry/virtualenvs/wav2vec-dsing-E_qMqI5b-py3.11/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 01:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>118.905500</td>\n",
       "      <td>27.841282</td>\n",
       "      <td>0.263889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>149.043100</td>\n",
       "      <td>25.975536</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>376.145800</td>\n",
       "      <td>26.509171</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>206.945200</td>\n",
       "      <td>27.088604</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 1.9081311305363973 minutes.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "trainer.train()\n",
    "finish = time.time()\n",
    "print(f\"Finished in {(finish-start)/60} minutes.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f95d4-f7a4-4b7a-96ac-6e9df08b0721",
   "metadata": {},
   "source": [
    "## Example Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cd33461-ee7f-444d-ab3f-d793070b556d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stop making a fol out of me',\n",
       " 'valerie',\n",
       " 'are you shoping anywhere',\n",
       " \"and i've mised your ginger hair\",\n",
       " 'and the way you like to dres',\n",
       " \"why don't you come on over valerie\",\n",
       " 'valerie valerie valerie',\n",
       " 'and in my head i paint a picture']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dsing_dataset['test']['labels']\n",
    "ref = asr_pipeline.tokenizer.batch_decode(labels)\n",
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4959593c-c64f-4130-bbd3-09ad272abf6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictions \u001b[38;5;241m=\u001b[39m beam_search_decoder(\u001b[43mlogits\u001b[49m)\n\u001b[1;32m      2\u001b[0m pred \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(prediction[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mwords) \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m predictions]\n\u001b[1;32m      3\u001b[0m pred\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logits' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = beam_search_decoder(logits)\n",
    "pred = [\" \".join(prediction[0].words) for prediction in predictions]\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60babd60-0306-41f1-9c55-827a4c80791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[1.,2.,3.,4.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b33b5fc-7c01-4496-96c0-ee3ecd76215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wav2vec dsing (Poetry)",
   "language": "python",
   "name": "w2v_dsing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
